---
title: "Breaking Mode Collapse: How Verbalized Sampling Restores LLM Creativity"
description: "Ask for a distribution, not a single answer. A training-free method to restore diversity in aligned LLMs."
slug: verbalized-sampling
pubDatetime: 2025-10-03T00:00:00Z
featured: true
draft: false
tags:
  - LLM
  - Diversity
  - Mode Collapse
  - Verbalized-Sampling
academic: true
authors:
  - name: "Jiayi Zhang*"
    aff: [1]
  - name: "Simon Yu*"
    aff: [1]
  - name: "Derek Chong*"
    aff: [2]
  - name: "Anthony Sicilia"
    aff: [3]
  - name: "Michael R. Tomz"
    aff: [2]
  - name: "Christopher D. Manning"
    aff: [2]
  - name: "Weiyan Shi"
    aff: [1]
affiliations:
  - "Northeastern University"
  - "Stanford University"
  - "West Virginia University"
pdfUrl: "/paper-preprint.pdf"
codeUrl: "https://github.com/CHATS-lab/verbalized-sampling"
bibUrl: "/references.bib"
abstract: "We prompt the model for a small distribution—k candidates plus weights—recovering base-model diversity while maintaining quality and safety across tasks."
showTOC: true
---

import VSPlayground from "@/components/interactives/VSPlayground";
import TypicalityBiasExplainer from "@/components/interactives/TypicalityBiasExplainer";
import TemperatureAblation from "@/components/interactives/TemperatureAblation";
import DiversityGainsVisual from "@/components/evidence/DiversityGainsVisual";
import PostTrainingVisual from "@/components/evidence/PostTrainingVisual";
import USStatesDemo from "@/components/evidence/USStatesDemo";
import ScalingTrendVisualization from "@/components/evidence/ScalingTrendVisualization";
import QuickStart from "@/components/sections/QuickStart";
import QualitativeExamples from "@/components/sections/QualitativeExamples";
import VSVariantsComparison from "@/components/sections/VSVariantsComparison";
import CodeBlock from "@/components/ui/CodeBlock";
import OpeningHook from "@/components/sections/OpeningHook";
import AhaMoment from "@/components/sections/AhaMoment";
import { Sidenote, Table } from "@/components/academic";
import Figure from "@/components/academic/Figure.astro";

{/* Badges removed for academic tone */}

<div className="not-prose mb-10">
  <OpeningHook client:visible />
  {/* Per IA: Opening Hook above TL;DR */}
</div>

## Intuition

Aligned LLMs can collapse to stereotypical outputs. VS asks for a small distribution (k items + probabilities) to recover diversity without training.

- Typicality bias ($\varepsilon>0$) sharpens the policy and induces collapse ($\rho = 1 + \varepsilon/\beta > 1$) [@rafailov2024; @ouyang2022].
- VS elicits a probability distribution over candidates [@zhang2025vs].
- A threshold $\tau$ tunes diversity [@zhang2025vs, pp. 7–8].

## Why It Works

### 1. Hidden bias in preference data

Human annotators systematically prefer familiar/typical text<Sidenote number={1}>Classic effects include mere exposure and processing fluency [@zajonc1968; @reber2004; @alter2009]. Statistical analyses indicate $\varepsilon>0$ independent of correctness (e.g., paired‑comparison style tests) [@ouyang2022; @christiano2017].</Sidenote>:

$$r(x,y) = r_{\text{true}}(x,y) + \varepsilon\,\log p_{\text{base}}(y|x).$$

### 2. Sharpening → mode collapse

<a id="eq:sharpening" />
$$\pi^*(y|x) \propto \pi_{\text{ref}}(y|x)^{\rho},\quad \rho = 1 + \varepsilon/\beta > 1.$$

When utility is flat across many valid completions, sharpening concentrates mass on typical outputs<Sidenote number={2}>Derivation follows standard KL‑regularized preference learning [@rafailov2024] with typicality‑biased rewards; see [@zhang2025vs, Eq. 3, p. 4].</Sidenote>.

### 3. Distribution‑level recovery (VS)

VS asks for a distribution of candidates with probabilities, recovering pretraining diversity. On the US states task, VS aligns with the corpus distribution ($\mathrm{KL}\approx 0.12$) [@zhang2025vs, p. 3].

Sharpening is defined in [Eq. @eq:sharpening](#eq:sharpening).

<Figure id="us-states" caption="Distribution recovery on US states (VS vs Direct); lower KL indicates closer match to the pretraining distribution [@zhang2025vs, Figure 2].">
  <USStatesDemo client:visible />
</Figure>

VS recovers the pretraining distribution closely<Sidenote number={3}>This KL divergence is measured against the empirical pretraining distribution for US states [@zhang2025vs, Figure 2, p. 3].</Sidenote>.

<div className="not-prose mb-10">
  <AhaMoment client:visible />
</div>

## Interactive Explainer: Typicality Bias

<div className="not-prose mb-12">
  <TypicalityBiasExplainer client:visible />
</div>

## Try It: Diversity Playground

Adjust the probability threshold τ to see how it affects which outputs are included. Lower thresholds = more diversity, higher thresholds = more focused outputs.

This threshold‑tuning effect corresponds to the paper’s ablations [@zhang2025vs, Figure 3g–i]<Sidenote number={4}>As τ increases, fewer low‑probability candidates are included; diversity decreases but focus increases. See [@zhang2025vs, Figure 3g–i].</Sidenote>.

<div className="not-prose mb-12">
  <VSPlayground client:visible />
</div>

## Evidence

VS increases diversity by **1.6–2.1×** across poem/story/joke tasks [@zhang2025vs, Figure 3a–c], improves human preference by **+25.7%** [@zhang2025vs, Table 3], and recovers **66.8%** of base‑model diversity after alignment [@zhang2025vs, Figure 4].

See [Figure @fig:diversity-gains](#fig:diversity-gains) and [Figure @fig:post-training](#fig:post-training) for details.

### Evidence at a Glance

- Creative writing diversity: +1.6–2.1× [@zhang2025vs, Figure 3a–c] → [Figure @fig:diversity-gains](#fig:diversity-gains)
- Post‑training retention: 66.8% [@zhang2025vs, Figure 4] → [Figure @fig:post-training](#fig:post-training)
- Temperature orthogonality: combines with VS [@zhang2025vs, Figure 5] → [Figure @fig:temperature-ablation](#fig:temperature-ablation)
- Scaling trend: larger models gain ~1.5–2× [@zhang2025vs, Figure 3e–f] → [Figure @fig:scaling-trend](#fig:scaling-trend)
- Dialogue simulation: VS matches/surpasses FT [@zhang2025vs, Figure 6a]
- Open‑ended QA: lower KL, higher coverage‑N, precision ≈ 1.0 [@zhang2025vs, Figure 7]
- Synthetic data → math accuracy: 37.5% vs 32.8% avg [@zhang2025vs, Table 4]

[Back to top ↑](#article)

<Figure id="diversity-gains" caption="Creative writing diversity improvements with VS (poem, story, joke); VS‑CoT achieves 1.6–2.1× gains [@zhang2025vs, Figure 3a–c].">
  <DiversityGainsVisual client:visible />
</Figure>

<Figure id="post-training" caption="Diversity retention across training stages; VS recovers to 66.8% of base diversity [@zhang2025vs, Figure 4].">
  <PostTrainingVisual client:visible />
</Figure>

### Open‑Ended QA (Fig. 7)

VS achieves lower KL divergence and higher coverage‑N while maintaining precision ≈ 1.0 on open‑ended QA benchmarks [@zhang2025vs, Figure 7]. This indicates distribution‑level prompting improves breadth without sacrificing answer quality.

<Table id="openqa" caption="Open‑ended QA metrics summary [@zhang2025vs, Figure 7].">
  <thead>
    <tr><th>Metric</th><th>VS impact</th></tr>
  </thead>
  <tbody>
    <tr><td>KL divergence</td><td>Lower (closer to target distribution)</td></tr>
    <tr><td>Coverage‑N</td><td>Higher (broader answer set)</td></tr>
    <tr><td>Precision</td><td>≈ 1.0 (maintained)</td></tr>
  </tbody>
  
</Table>

See [Table @table:openqa](#table:openqa) for a compact summary.

### Synthetic Data → Math Accuracy (Table 4)

Training with VS‑generated synthetic data improves downstream math accuracy (**37.5% vs 32.8%** average), demonstrating transfer beyond creative tasks [@zhang2025vs, Table 4].

<Table id="synthetic-math" caption="Synthetic data → downstream math accuracy [@zhang2025vs, Table 4].">
  <thead>
    <tr><th>Training data</th><th>Average accuracy</th></tr>
  </thead>
  <tbody>
    <tr><td>VS‑generated synthetic</td><td>37.5%</td></tr>
    <tr><td>Baseline</td><td>32.8%</td></tr>
  </tbody>
</Table>

See [Table @table:synthetic-math](#table:synthetic-math) for key figures.

## VS ≠ Temperature: Orthogonal Benefits

Temperature changes how you sample from a distribution; VS changes what distribution you request. They combine effectively [@zhang2025vs, Figure 5].

See [Figure @fig:temperature-ablation](#fig:temperature-ablation) for the combined diversity–quality Pareto frontier<Sidenote number={5}>VS shifts the underlying distribution, while temperature adjusts sampling from that distribution; their effects are complementary [@zhang2025vs, Figure 5].</Sidenote>.

<Figure id="temperature-ablation" caption="VS is orthogonal to temperature; combining the two improves the diversity–quality frontier [@zhang2025vs, Figure 5].">
  <TemperatureAblation client:visible />
</Figure>

## Larger Models Benefit More

Larger models show stronger gains with VS<Sidenote number={6}>Scaling analysis indicates ~1.5–2× larger diversity gains for higher‑capacity models [@zhang2025vs, Figure 3e–f, p. 7].</Sidenote>.

<Figure id="scaling-trend" caption="Larger models benefit ~1.5–2× more from VS [@zhang2025vs, Figure 3e–f].">
  <ScalingTrendVisualization client:visible />
</Figure>

## Dialogue Simulation

In dialogue simulation tasks, GPT‑4 with VS matches a fine‑tuned Llama‑3.1‑8B model, and DeepSeek‑R1 with VS surpasses it [@zhang2025vs, Figure 6a, p. 11]. This illustrates VS’s applicability beyond creative generation into behaviorally grounded distributions.

See also: [Prompt Recipes](#recipes) and [VS Variants](#variants).

[Back to top ↑](#article)

<a id="variants" />
## VS Variants: When to Use Which

<div className="not-prose mb-12">
  <VSVariantsComparison client:visible />
</div>

Per Appendix H.3, “probability” performs best for VS‑Standard/CoT, while “confidence” is used in VS‑Multi [@zhang2025vs].

[Back to top ↑](#article)

## Qualitative Examples

See the difference in actual outputs between direct prompting and VS:

<div className="not-prose mb-12">
  <QualitativeExamples client:visible />
</div>

## Is VS Right for You?

Use VS when:
- You need creative diversity (stories, jokes, ideas)
- You want realistic distributions (simulations, surveys)
- You are generating synthetic data and want variety with quality
- You prefer training‑free techniques compatible with closed models

Skip VS when:
- There is a single correct answer or strict determinism is required
- Maximal speed or minimum token usage is the only priority

<a id="recipes" />
## Prompt Recipes

<div className="not-prose mb-6">
  <QuickStart client:idle />
  {/* Quick Start embedded here for copy‑ready flow */}
  </div>

### VS‑Standard (JSON)

<CodeBlock
  code={`Generate k={5} {TASK} with their probabilities.\nReturn JSON: {\"candidates\":[{\"text\":\"...\",\"prob\":0.28}, ...]}\nOnly include candidates with probability ≥ {τ}. Ensure probabilities sum to 1.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-Standard' }}
  client:idle
/>

### VS‑CoT

<CodeBlock
  code={`Think step-by-step to enumerate distinct styles/approaches.\nThen generate k={5} {TASK} with probabilities in JSON (probabilities sum to 1).\nOnly include items with probability ≥ {τ}.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-CoT' }}
  client:idle
/>

### VS‑Multi (confidence)

<CodeBlock
  code={`Generate k={5} {TASK} candidates.\nFor each, return text and confidence ∈ [0,1]. Only include items with confidence ≥ {τ_conf}.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-Multi' }}
  client:idle
/>

See the [variants comparison](#variants) for when to use each variant.

## Implementation Tips

Do:

- Use `k=5` for quality/diversity; larger `k` often degrades quality<Sidenote number={7}>Empirically, diminishing returns beyond k=5 with quality degradation for k>10 [@zhang2025vs, Appendix H.1].</Sidenote>.
- Ask for “probability” (not “likelihood”) in VS‑Standard/CoT.
- Specify JSON and ensure probabilities sum to 1.
- Use a $\tau$ threshold for diversity control.

Don’t:

- Use `k>10` unless necessary.
- Mix “probability” and “confidence” terms.
- Forget normalization.
- Apply VS to strictly factual/math tasks.

## Frequently Asked Questions

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">Does VS hurt accuracy or safety?</summary>
  <p className="mt-3 text-gray-600">
    No. The paper shows VS maintains factual accuracy (Appendix G.7) and safety (Appendix G.8) [@zhang2025vs].
    It only increases diversity for tasks with multiple valid answers.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">What is semantic diversity?</summary>
  <p className="mt-3 text-gray-600">
    Semantic diversity = $1 - \mathrm{mean}(\mathrm{cosine\_similarity})$. It measures how different
    the meanings are across generated responses, not just surface-level word differences.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">Why not just use temperature?</summary>
  <p className="mt-3 text-gray-600">
    Temperature and VS are orthogonal. Temperature affects sampling randomness from the same
    distribution, while VS changes the distribution itself [@zhang2025vs, Figure 5]. Combining them gives best results.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-12">
  <summary className="font-semibold cursor-pointer">Which models support VS?</summary>
  <p className="mt-3 text-gray-600">
    Any instruction-following LLM: GPT-4, Claude, Gemini, Llama, Qwen, and even reasoning
    models like o1 and DeepSeek R1. No special access or modifications needed.
  </p>
</details>

## References

<section id="references" />
