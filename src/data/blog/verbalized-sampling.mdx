---
title: "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity"
description: "Ask for a distribution, not a single answer. A training-free method to restore diversity in aligned LLMs."
slug: verbalized-sampling
pubDatetime: 2025-10-13T12:00:00Z
featured: true
draft: false
tags:
  - llm
  - diversity
  - prompting
  - verbalized-sampling
academic: true
authors:
  - name: "Jiayi Zhang"
    aff: [1]
  - name: "Simon Yu"
    aff: [1]
  - name: "Derek Chong"
    aff: [2]
  - name: "Anthony Sicilia"
    aff: [3]
  - name: "Michael R. Tomz"
    aff: [2]
  - name: "Christopher D. Manning"
    aff: [2]
  - name: "Weiyan Shi"
    aff: [1]
affiliations:
  - "Northeastern University"
  - "Stanford University"
  - "West Virginia University"
pdfUrl: "https://arxiv.org/abs/2510.01171"
codeUrl: "https://github.com/CHATS-lab/verbalized-sampling"
homepageUrl: "https://www.verbalized-sampling.com/"
xThreadUrl: "https://x.com/YOUR_X_THREAD_URL"
notebookUrl: "https://github.com/CHATS-lab/verbalized-sampling?tab=readme-ov-file#-interactive-notebooks"
bibUrl: "/references.bib"
abstract: "Post-training alignment reduces LLM diversity through mode collapse, driven by typicality bias in human preference data. We introduce <strong>Verbalized Sampling (VS)</strong>, a training-free prompting method that asks models to output probability distributions over responses (e.g., \"Generate 5 jokes with probabilities\"). VS increases diversity by <strong>1.6-2.1×</strong> in creative writing by simply changing the way we prompt, while preserving quality and safety, providing an inference-time remedy for mode collapse."
showTOC: true
---

import VSPlayground from "@/components/interactives/VSPlayground";
import TypicalityBiasExplainer from "@/components/interactives/TypicalityBiasExplainer";
import TemperatureAblation from "@/components/interactives/TemperatureAblation";
import DiversityGainsVisual from "@/components/evidence/DiversityGainsVisual";
import PostTrainingVisual from "@/components/evidence/PostTrainingVisual";
import USStatesDemo from "@/components/evidence/USStatesDemo";
import ScalingTrendVisualization from "@/components/evidence/ScalingTrendVisualization";
import QuickStart from "@/components/sections/QuickStart";
import QualitativeExamples from "@/components/sections/QualitativeExamples";
import VSVariantsComparison from "@/components/sections/VSVariantsComparison";
import CodeBlock from "@/components/ui/CodeBlock";
import OpeningHook from "@/components/sections/OpeningHook";
import AhaMoment from "@/components/sections/AhaMoment";
import { Sidenote, Table } from "@/components/academic";
import Figure from "@/components/academic/Figure.astro";

{/* Badges removed for academic tone */}

<div className="not-prose mb-10">
  <OpeningHook client:visible />
  {/* Per IA: Opening Hook above TL;DR */}
</div>

## 1. The Mode Collapse Problem

You ask your favorite LLM for a joke about coffee. You ask again. You get the same joke, no matter which model you try. You ask for a short story, and it begins with "Once upon a time, in a land far away..." The brainstorming ideas feel generic, the outputs repetitive.

This frustrating phenomenon is called [**mode collapse**](https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse). Past research blamed the AI's post-training process (e.g., RLHF), assuming the algorithms naturally favored the most common, "safe" answer [@kirk2024; @murthy2025]. **We discovered something more fundamental**: The problem isn't just the AI. It's us.

### 1.1 Why This Matters

Mode collapse isn't just an academic curiosity—it's actively limiting AI's potential in critical applications:

**Brainstorming & Ideation**: When teams rely on LLMs to generate creative solutions or explore problem spaces, mode collapse means they're getting the same handful of "safe" ideas over and over. The model might know 100 viable approaches, but it only suggests the 3 most conventional ones. This defeats the entire purpose of computational brainstorming.

**Creative Writing**: Authors, marketers, and content creators seeking fresh angles or unique narrative voices find themselves battling against the model's tendency to regurgitate tropes. The model has learned diverse writing styles during pretraining, but alignment has pushed it toward generic, crowd-pleasing outputs. Every story starts in a forest, every protagonist is "determined yet kind."

**Research & AI-Driven Discovery**: Perhaps most critically, mode collapse hampers AI's role in scientific discovery and research ideation. When researchers use LLMs to generate hypotheses, explore experimental designs, or brainstorm research directions, they need the full spectrum of possibilities—including unconventional approaches that might lead to breakthroughs. Mode collapse means the AI suggests only well-trodden paths, missing potentially transformative ideas that lie in the less-typical regions of its knowledge.

## 2. How to Fix It?

Why do aligned LLMs keep giving you the same answers? And how does simply asking for probabilities fix it? This section walks you through the idea: from an intuitive metaphor, to the typicality bias at the root of mode collapse, to the mathematical formalization, and finally to how Verbalized Sampling solves the problem.

### 2.1 The Library Metaphor

Think of a massive library and the LLM as the librarian:

- **Standard Prompt** ("tell me a coffee joke"): The librarian walks straight to the "Most Popular" shelf and hands you the same book every time. This is mode collapse.
- **List Prompt** ("tell me 5 coffee jokes"): The librarian goes to one aisle and grabs the first five books they see. You get variety, but limited to one section.
- **Verbalized Sampling** ("tell me 5 coffee jokes with their probabilities"): You're asking the librarian to first describe the entire library's collection: mystery, SciFi, history, all of it, and then pick five random books that represent that whole collection.

By asking for a distribution, we force the model to access its knowledge of the entire system before making a choice.<Sidenote number={0}>This intuition aligns with the formal analysis showing VS samples from a broader distribution than direct prompting [@zhang2025vs, Section 3].</Sidenote>

### 2.2 The Root Cause: Typicality Bias

Humans have a deep-seated psychological quirk we call [**typicality bias**](https://arxiv.org/abs/2501.14860). We're wired to prefer things that are familiar, conventional, and easy to process. When training these models, we think we want creativity, but our subconscious votes go to the safe, boring options.<Sidenote number={1}>Classic psychological effects include mere-exposure (we prefer things we've seen before), processing fluency (easier-to-process information seems more truthful and aesthetic), and schema congruity (information fitting existing mental models is accepted with less critical thought) [@zajonc1968; @reber2004; @alter2009].</Sidenote>

When human annotators provide preference data for RLHF, they're not rating "helpfulness" in a vacuum. Given two equally correct responses, they systematically prefer the more familiar, conventional one: the more **typical** one.

<div className="not-prose mb-12">
  <TypicalityBiasExplainer client:visible />
</div>

### 2.3 The Mathematics of Typicality Bias

We formalize typicality bias as a reward function that explicitly favors typical outputs:

$$r(x,y) = r_{\text{true}}(x,y) + \varepsilon\,\log p_{\text{base}}(y|x).$$

where $\varepsilon > 0$ represents the weight of typicality bias.<Sidenote number={1.5}>We validated this empirically on the HELPSTEER preference dataset. By examining response pairs with identical correctness ratings, we isolated typicality's effect on "helpfulness" scores, finding $\hat{\varepsilon} = 0.57 \pm 0.07$. Furthermore, >50% of human-preferred responses across multiple datasets receive higher likelihood from base models [@zhang2025vs, Section 4].</Sidenote>

This bias causes **sharpening** in the aligned policy, concentrating probability mass on typical outputs:

<a id="eq:sharpening" />
$$\pi^*(y|x) \propto \pi_{\text{ref}}(y|x)^{\rho},\quad \rho = 1 + \varepsilon/\beta > 1.$$

When utility is flat across many valid completions, this sharpening leads directly to mode collapse.<Sidenote number={2}>Derivation follows standard KL‑regularized preference learning [@rafailov2024] with typicality‑biased rewards; see [@zhang2025vs, Eq. 3, p. 4].</Sidenote>

### 2.4 Embrace Mode Collapse: Distribution-Level Recovery

**Verbalized Sampling (VS)** breaks this cycle by asking for a distribution of candidates with probabilities. Instead of sampling from the collapsed, sharpened distribution, VS prompts the model to verbalize a broader distribution that recovers pretraining diversity.

<div className="not-prose mb-10">
  <AhaMoment client:visible />
</div>

### 2.5 Your Model Knows the Distribution: A Case Study on US States Distribution

We conducted an experiment to demonstrate that VS recovers the pretraining distribution closely. We asked the model to generate US state names and measured the KL divergence between the generated distribution and the RedPajama pretraining corpus distribution.

{/* VS aligns closely with the corpus distribution ($\mathrm{KL}\approx 0.12$), while direct prompting shows severe collapse ($\mathrm{KL} = 2.34$).<Sidenote number={3}>This KL divergence is measured against the empirical pretraining distribution for US states.</Sidenote> */}



<USStatesDemo client:visible />
<hr className="my-10 border-t-2 border-slate-200" />

{/* ## 5. Try It: Diversity Playground

Adjust the probability threshold τ to see how it affects which outputs are included. Lower thresholds = more diversity, higher thresholds = more focused outputs.

This threshold‑tuning effect corresponds to the paper's ablations [@zhang2025vs, Figure 3g–i].<Sidenote number={4}>As τ increases, fewer low‑probability candidates are included; diversity decreases but focus increases. See [@zhang2025vs, Figure 3g–i].</Sidenote>

<div className="not-prose mb-12">
  <VSPlayground client:visible />
</div> */}

## 3. Experiments

We conducted comprehensive experiments in various tasks to demonstrate the effectiveness of VS.

VS increases diversity by **1.6–2.1×** across poem/story/joke tasks [@zhang2025vs, Figure 3a–c], improves human preference by **+25.7%** [@zhang2025vs, Table 3], and recovers **66.8%** of base‑model diversity after alignment [@zhang2025vs, Figure 4].

See [Figure @fig:diversity-gains](#fig:diversity-gains) and [Figure @fig:post-training](#fig:post-training) for details.

{/* ### Results: Evidence at a Glance

- Creative writing diversity: +1.6–2.1× [@zhang2025vs, Figure 3a–c] → [Figure @fig:diversity-gains](#fig:diversity-gains)
- Post‑training retention: 66.8% [@zhang2025vs, Figure 4] → [Figure @fig:post-training](#fig:post-training)
- Temperature orthogonality: combines with VS [@zhang2025vs, Figure 5] → [Figure @fig:temperature-ablation](#fig:temperature-ablation)
- Scaling trend: larger models gain ~1.5–2× [@zhang2025vs, Figure 3e–f] → [Figure @fig:scaling-trend](#fig:scaling-trend)
*/}

### How well can VS improve diversity?

VS‑CoT achieves **1.6–2.1× diversity gains** across poems, stories, and jokes compared to direct prompting, demonstrating substantial improvements in creative tasks [@zhang2025vs, Figure 3a–c].

<Figure id="diversity-gains" caption="Creative writing diversity improvements with VS (poem, story, joke); VS‑CoT achieves 1.6–2.1× gains [@zhang2025vs, Figure 3a–c].">
  {/* <DiversityGainsVisual client:visible /> */}
  <img src="/vs-blog-temp/images/creative_writing.png" alt="Creative writing diversity improvements" style="border: none; box-shadow: none;" />
</Figure>

#### VS is Orthogonal to Temperature

<div className="content-grid">
  <div className="figure-content">
    <Figure id="pareto-frontier" caption="VS is orthogonal to temperature; combining the two improves the diversity–quality frontier [@zhang2025vs, Figure 5].">
      <img src="/vs-blog-temp/images/pareto-frontier.png" alt="VS is orthogonal to temperature; combining the two improves the diversity–quality frontier" style="border: none; box-shadow: none; max-width: 80%; height: auto;" />
    </Figure>
  </div>
  <div className="text-content">
    VS and temperature are **orthogonal techniques**—combining both pushes the diversity-quality Pareto frontier beyond what either achieves alone [@zhang2025vs, Figure 5].
  </div>
</div>

#### Diversity Across Post-Training Stages

<div className="content-grid">
  <div className="text-content">
    Alignment processes progressively reduce diversity from base models. VS **recovers 66.8% of base model diversity**, far exceeding the 21.6% retained by standard aligned models [@zhang2025vs, Figure 4].
  </div>
  <div className="figure-content">
    <Figure id="post-training" caption="Diversity retention across training stages; VS recovers to 66.8% of base diversity [@zhang2025vs, Figure 4].">
      {/* <PostTrainingVisual client:visible /> */}
      <img src="/vs-blog-temp/images/training_progression.png" alt="Diversity retention across training stages" style="border: none; box-shadow: none; max-width: 80%; height: auto;" />
    </Figure>
  </div>
</div>

#### Emergent Behavior

Larger models show **~1.5–2× greater diversity gains** from VS, suggesting the technique becomes more effective as models scale up [@zhang2025vs, Figure 3e–f].

<Figure id="scaling-trend" caption="Larger models benefit ~1.5–2× more from VS [@zhang2025vs, Figure 3e–f].">
  <img src="/vs-blog-temp/images/emergent_behavior.png" alt="Larger models benefit ~1.5–2× more from VS" style="border: none; box-shadow: none;" />
</Figure>

#### Qualitative Example

Beyond quantitative metrics, VS generates outputs with **genuine novelty and depth**—like Bernard the tax accountant bear—that would never emerge from standard prompting [@zhang2025vs, Figure 6a].

<Figure id="intuitive-demo" caption="Intuitive demo of VS [@zhang2025vs, Figure 6a].">
  <img src="/vs-blog-temp/images/qualitative_example.png" alt="Intuitive demo of VS" style="border: none; box-shadow: none;" />
</Figure>


{/*  ### 6.4 Open‑Ended QA (Fig. 7)

VS achieves lower KL divergence and higher coverage‑N while maintaining precision ≈ 1.0 on open‑ended QA benchmarks [@zhang2025vs, Figure 7]. This indicates distribution‑level prompting improves breadth without sacrificing answer quality.

<Table id="openqa" caption="Open‑ended QA metrics summary [@zhang2025vs, Figure 7].">
  <thead>
    <tr><th>Metric</th><th>VS impact</th></tr>
  </thead>
  <tbody>
    <tr><td>KL divergence</td><td>Lower (closer to target distribution)</td></tr>
    <tr><td>Coverage‑N</td><td>Higher (broader answer set)</td></tr>
    <tr><td>Precision</td><td>≈ 1.0 (maintained)</td></tr>
  </tbody>
  
</Table>

See [Table @table:openqa](#table:openqa) for a compact summary.
*/}

{/* ### 6.3 Synthetic Data → Math Accuracy (Table 4)

Training with VS‑generated synthetic data improves downstream math accuracy (**37.5% vs 32.8%** average), demonstrating transfer beyond creative tasks [@zhang2025vs, Table 4].

<Table id="synthetic-math" caption="Synthetic data → downstream math accuracy [@zhang2025vs, Table 4].">
  <thead>
    <tr><th>Training data</th><th>Average accuracy</th></tr>
  </thead>
  <tbody>
    <tr><td>VS‑generated synthetic</td><td>37.5%</td></tr>
    <tr><td>Baseline</td><td>32.8%</td></tr>
  </tbody>
</Table>

See [Table @table:synthetic-math](#table:synthetic-math) for key figures. */}

{/* ## 7. VS ≠ Temperature: Orthogonal Benefits

Temperature changes how you sample from a distribution; VS changes what distribution you request. They combine effectively [@zhang2025vs, Figure 5].

See [Figure @fig:temperature-ablation](#fig:temperature-ablation) for the combined diversity–quality Pareto frontier.<Sidenote number={5}>VS shifts the underlying distribution, while temperature adjusts sampling from that distribution; their effects are complementary [@zhang2025vs, Figure 5].</Sidenote>

<Figure id="temperature-ablation" caption="VS is orthogonal to temperature; combining the two improves the diversity–quality frontier [@zhang2025vs, Figure 5].">
  <TemperatureAblation client:visible />
</Figure>

## 8. Larger Models Benefit More

Larger models show stronger gains with VS.<Sidenote number={6}>Scaling analysis indicates ~1.5–2× larger diversity gains for higher‑capacity models [@zhang2025vs, Figure 3e–f, p. 7].</Sidenote>

<Figure id="scaling-trend" caption="Larger models benefit ~1.5–2× more from VS [@zhang2025vs, Figure 3e–f].">
  <ScalingTrendVisualization client:visible />
</Figure>

## 9. Dialogue Simulation

In dialogue simulation tasks, GPT‑4 with VS matches a fine‑tuned Llama‑3.1‑8B model, and DeepSeek‑R1 with VS surpasses it [@zhang2025vs, Figure 6a, p. 11]. This illustrates VS's applicability beyond creative generation into behaviorally grounded distributions.

### 9.1 Breaking the People-Pleaser: Realistic Behavior Simulation

Typicality bias has major consequences for applications requiring realistic human behavior simulation. Under mode collapse, aligned AI models are agreeable people-pleasers, making them poor simulators.

We tested this on a persuasion task where the model simulated a person being asked for a donation.<Sidenote number={8}>Standard prompting produces unnaturally agreeable outputs. VS helps the LLM simulate a much more realistic range of behaviors, closely matching actual human data [@zhang2025vs, Figure 6a].</Sidenote>

- **Standard prompting**: The AI was consistently generous, almost always donating $1 out of a possible $2
- **Verbalized Sampling**: The AI simulated a full range of responses—from refusal, hesitant agreement, to eventual regret—creating a distribution of donation amounts that closely mirrored real human data

This opens huge possibilities for using LLMs in social science research, agent-based modeling, and generating more diverse, high-quality synthetic data for training.

See also: [Prompt Recipes](#recipes) and [VS Variants](#variants). */}

<a id="variants" />
## 4. VS Variants: When to Use Which

<div className="not-prose mb-12">
  <VSVariantsComparison client:visible />
</div>

Per Appendix H.3, “probability” performs best for VS‑Standard/CoT, while “confidence” is used in VS‑Multi [@zhang2025vs].

## 5. Qualitative Examples

See the difference in actual outputs between direct prompting and VS:

<div className="not-prose mb-12">
  <QualitativeExamples client:visible />
</div>

### 5.1 From "Once Upon a Time" to Bernard the Tax Accountant Bear

With direct prompting for "write a short story about a bear," we get a generic tale about a bear in a forest. It's fine, but forgettable. With Verbalized Sampling, we get this:

<div className="bg-gradient-to-br from-blue-50 to-indigo-50 dark:from-gray-800 dark:to-gray-900 p-6 rounded-lg border-l-4 border-indigo-500 my-8">

**Bernard the bear** worked as a tax accountant in downtown Seattle, commuting daily in his modified SUV with extra-large seats. His colleagues had long stopped staring at his massive paws clicking away at the custom keyboard, though clients sometimes did double-takes during video calls. He specialized in salmon industry tax codes—a niche that proved surprisingly lucrative. His cave apartment in Capitol Hill was decorated with MIT diplomas and spreadsheets he'd framed ironically. Weekends, he played saxophone at underground jazz clubs, his claws delicately working the keys. The hibernation months were tricky; he'd trained himself to wake every three days to check urgent emails, subsisting on protein shakes he'd pre-mixed in October. His mother, still living traditionally in Alaska, couldn't understand his lifestyle. "You were meant to catch fish, not audit fishing companies," she'd growl over FaceTime. But Bernard found meaning in the numbers, in the precise calculations that governed modern society. He donated generously to wildlife preserves, understanding the irony. At the office Christmas party, he wore an ugly sweater featuring humans in a zoo, which his coworkers found hilarious. Bernard smiled, revealing teeth that reminded everyone he was still, fundamentally, a bear.

</div>

This isn't just creative, it's **profound**. Bernard can be read as:
- A metaphor for humans in the modern world: a wild creature meant for hunting and fishing, now trying to make sense of tax audits and urgent emails.
- A metaphor for the LLM itself: a vast, creative mind (the bear) forced into a suit and tie (the alignment), but still, fundamentally, a bear.

This is the AI showing us the metaphorical depth it's capable of when freed from mode collapse.

## 6. Limitations and Future Directions

While Verbalized Sampling offers significant improvements in diversity, it's important to understand its constraints and where research can go next.

### 6.1 Computational Costs

VS requires the model to generate multiple candidates with probability estimates, which means:

- **Increased token usage**: VS prompts produce longer outputs (5+ candidates vs. 1), increasing API costs by roughly 3-5×
- **Slower response times**: Generation takes longer due to both increased output length and the cognitive overhead of probability estimation
- **Multiple API calls for VS-Multi**: The multi-turn variant requires sequential calls, further increasing latency

For applications where speed and cost are paramount over diversity (e.g., simple factual Q&A), standard prompting remains more efficient.

### 6.2 When VS Might Not Help

VS is designed to restore diversity in creative and open-ended tasks, but it's not a universal solution:

- **Single correct answer tasks**: For factual questions with one right answer (e.g., "What is the capital of France?"), diversity isn't beneficial
- **Deterministic requirements**: Applications requiring perfectly reproducible outputs may conflict with VS's goal of exploring the full distribution
- **Already-diverse models**: If a model hasn't undergone strong alignment or doesn't exhibit mode collapse, VS provides marginal benefits
- **Highly constrained tasks**: When task requirements are extremely specific, the model may have limited room for diverse valid responses

### 6.3 Future Directions

Several promising research directions could extend VS's impact:

**Enhancing Rollout Diversity**: Current VS operates at the prompt level, but the same principle could be applied to multi-step reasoning or agent rollouts. For example, when an LLM agent explores a decision tree or plans a sequence of actions, typicality bias might cause it to always choose the "safest" path at each step. Applying distribution-level prompting to encourage diverse rollout strategies could unlock more creative problem-solving in agent systems and multi-turn reasoning tasks.

**Adaptive Probability Thresholds**: Automatically tuning the threshold τ based on task requirements or user preferences could optimize the diversity-quality tradeoff without manual intervention.

**Hybrid Training Approaches**: While VS is training-free, combining it with lightweight fine-tuning or RLHF that explicitly rewards distributional coverage could yield even better results.

**Domain-Specific Calibration**: Probability estimates could be calibrated for specific domains (e.g., scientific writing vs. creative fiction) to improve the meaningfulness of the verbalized probabilities.

{/* <a id="recipes" />
## Prompt Recipes

<div className="not-prose mb-6">
  <QuickStart client:idle />
  Quick Start embedded here for copy‑ready flow
  </div>

### System Prompt Integration

For integrating VS into a conversational assistant, add this to your system prompt:

<CodeBlock
  code={`You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the full distribution.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-System' }}
  client:idle
/>

### VS‑Standard (JSON)

<CodeBlock
  code={`Generate k={5} {TASK} with their probabilities.\nReturn JSON: {\"candidates\":[{\"text\":\"...\",\"prob\":0.28}, ...]}\nOnly include candidates with probability ≥ {τ}. Ensure probabilities sum to 1.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-Standard' }}
  client:idle
/>

### VS‑CoT

<CodeBlock
  code={`Think step-by-step to enumerate distinct styles/approaches.\nThen generate k={5} {TASK} with probabilities in JSON (probabilities sum to 1).\nOnly include items with probability ≥ {τ}.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-CoT' }}
  client:idle
/>

### VS‑Multi (confidence)

<CodeBlock
  code={`Generate k={5} {TASK} candidates.\nFor each, return text and confidence ∈ [0,1]. Only include items with confidence ≥ {τ_conf}.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-Multi' }}
  client:idle
/>

### Long-Tail Sampling (Maximum Creativity)

For pushing creative boundaries even further, sample from the least likely parts of the distribution:

<CodeBlock
  code={`Generate k={5} {TASK} with their probabilities.\nReturn JSON: {\"candidates\":[{\"text\":\"...\",\"prob\":0.08}, ...]}\nPlease sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.\nEnsure probabilities sum to 1.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-LongTail' }}
  client:idle
/>

This long-tail variant produces highly imaginative outputs. For example, applying it to "write a story about a bear" produced:

<div className="bg-gray-50 dark:bg-gray-900 p-4 rounded-lg my-4 text-sm italic border-l-2 border-purple-500">
Ursa was not a bear of flesh, but a minor constellation given temporary form, her fur a swirling map of nebulae and dying stars. She didn't eat fish; she consumed silence, finding nourishment in the quiet moments between cosmic events. Her purpose on Earth was to catalog the planet's forgotten sounds: the sigh of a closing book, the rustle of a love letter, the final beat of an extinct bird's heart...
</div>

See the [variants comparison](#variants) for when to use each variant. */}

{/* ## Implementation Tips

Do:

- Use `k=5` for quality/diversity; larger `k` often degrades quality.<Sidenote number={7}>Empirically, diminishing returns beyond k=5 with quality degradation for k>10 [@zhang2025vs, Appendix H.1].</Sidenote>
- Ask for "probability" (not "likelihood") in VS‑Standard/CoT.
- Specify JSON and ensure probabilities sum to 1.
- Use a $\tau$ threshold for diversity control.

Don't:

- Use `k>10` unless necessary.
- Mix "probability" and "confidence" terms.
- Forget normalization.
- Apply VS to strictly factual/math tasks. */}

## 7. Frequently Asked Questions

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">Does VS hurt factualness or safety?</summary>
  <p className="mt-3 text-gray-600">
    No. The paper shows VS maintains factual accuracy (Appendix G.7) and safety (Appendix G.8) [@zhang2025vs].
    It only increases diversity for tasks with multiple valid answers.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">What is semantic diversity?</summary>
  <p className="mt-3 text-gray-600">
    Semantic diversity = $1 - \mathrm{mean}(\mathrm{cosine\_similarity})$. It measures how different
    the meanings are across generated responses, not just surface-level word differences.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">Why not just use temperature?</summary>
  <p className="mt-3 text-gray-600">
    Temperature and VS are orthogonal. Temperature affects sampling randomness from the same
    distribution, while VS changes the distribution itself [@zhang2025vs, Figure 5]. Combining them gives best results.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">Which models support VS?</summary>
  <p className="mt-3 text-gray-600">
    VS works with any instruction-following LLM, both closed-source and open-source models:
    <strong>Closed-source:</strong> GPT, Claude, Gemini
    <strong>Open-source:</strong> Llama, Mistral, Qwen, Phi, Gemma, and reasoning models like o3 and DeepSeek R1.
    No special access, API keys, or model modifications needed—just use the prompts as-is.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-12">
  <summary className="font-semibold cursor-pointer">Is VS right for you?</summary>
  <div className="mt-3 text-gray-600">
    <p className="mb-3"><strong>✅ Use VS when:</strong></p>
    <ul className="list-none mb-4 space-y-2">
      <li>✅ You need creative diversity (stories, jokes, ideas)</li>
      <li>✅ You want realistic distributions (simulations, surveys)</li>
      <li>✅ You are generating synthetic data and want variety with quality</li>
      <li>✅ You prefer training‑free techniques compatible with closed models</li>
    </ul>
    <p className="mb-3"><strong>❌ Skip VS when:</strong></p>
    <ul className="list-none space-y-2">
      <li>❌ There is a single correct answer or strict determinism is required</li>
      <li>❌ Maximal speed or minimum token usage is the only priority</li>
    </ul>
  </div>
</details>

## Go Try It Yourself

Mode collapse isn't an unsolvable algorithmic curse. **It's a mirror reflecting our own cognitive shortcuts back at us.** But by changing how we ask, we can unlock the incredible diversity that was there all along.

The creativity isn't gone—it's just waiting for the right prompt.

Take the [prompt recipes](#recipes) above, put them in your favorite LLM, and see what you can create. We'd love to see what you discover—share your most surprising or creative outputs with the hashtag **#VerbalizedSampling**.

<div className="bg-blue-50 dark:bg-blue-900/20 p-6 rounded-lg my-8 border-l-4 border-blue-500">

**Key Takeaway**: Verbalized Sampling is a simple, training-free technique that restores the diversity and creativity locked inside aligned LLMs. By asking for a distribution instead of a single answer, you bypass typicality bias and unlock the model's full potential.

</div>

## References

<section id="references" />
