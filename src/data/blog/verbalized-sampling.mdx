---
title: "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity"
description: "Blog showcasing Verbalized Sampling—a training-free method that restores diversity in aligned LLMs by 1.6-2.1× by asking for probability distributions instead of single answers. Explore results, examples, and try it yourself."
slug: verbalized-sampling
pubDatetime: 2025-10-13T12:00:00Z
featured: true
draft: false
tags:
  - LLM
  - Diversity
  - Mode Collapse
  - Verbalized-Sampling
academic: true
authors:
  - name: "Jiayi Zhang"
    aff: [1]
  - name: "Simon Yu"
    aff: [1]
  - name: "Derek Chong"
    aff: [2]
  - name: "Anthony Sicilia"
    aff: [3]
  - name: "Michael R. Tomz"
    aff: [2]
  - name: "Christopher D. Manning"
    aff: [2]
  - name: "Weiyan Shi"
    aff: [1]
affiliations:
  - "Northeastern University"
  - "Stanford University"
  - "West Virginia University"
pdfUrl: "https://arxiv.org/abs/2510.01171"
codeUrl: "https://github.com/CHATS-lab/verbalized-sampling"
homepageUrl: "https://www.verbalized-sampling.com/"
xThreadUrl: "https://x.com/YOUR_X_THREAD_URL"
notebookUrl: "https://github.com/CHATS-lab/verbalized-sampling?tab=readme-ov-file#-interactive-notebooks"
bibUrl: "/references.bib"
abstract: "Post-training alignment reduces LLM diversity through mode collapse, driven by typicality bias in human preference data. We introduce <strong>Verbalized Sampling (VS)</strong>, a training-free prompting method that asks models to output probability distributions over responses (e.g., \"Generate 5 jokes with probabilities\"). VS increases diversity by <strong>1.6-2.1×</strong> in creative writing by simply changing the way we prompt, while preserving quality and safety, providing an inference-time remedy for mode collapse."
showTOC: true
---

import VSPlayground from "@/components/interactives/VSPlayground";
import TypicalityBiasExplainer from "@/components/interactives/TypicalityBiasExplainer";
import TemperatureAblation from "@/components/interactives/TemperatureAblation";
import DiversityGainsVisual from "@/components/evidence/DiversityGainsVisual";
import PostTrainingVisual from "@/components/evidence/PostTrainingVisual";
import USStatesDemo from "@/components/evidence/USStatesDemo";
import ScalingTrendVisualization from "@/components/evidence/ScalingTrendVisualization";
import QuickStart from "@/components/sections/QuickStart";
import QualitativeExamples from "@/components/sections/QualitativeExamples";
import VSVariantsComparison from "@/components/sections/VSVariantsComparison";
import CodeBlock from "@/components/ui/CodeBlock";
import OpeningHook from "@/components/sections/OpeningHook";
import AhaMoment from "@/components/sections/AhaMoment";
import { Sidenote, Table, Equation } from "@/components/academic";
import Figure from "@/components/academic/Figure.astro";

{/* Badges removed for academic tone */}
<hr class="my-10 border-border opacity-80" />

<div className="not-prose mb-10">
  <OpeningHook client:visible />
  {/* Per IA: Opening Hook above TL;DR */}
</div>

## The Mode Collapse Problem

You ask your favorite LLM for a joke about coffee. You ask again. You get the same joke, no matter which model you try. You ask for a short story, and it begins with "Once upon a time, in a land far away..." The brainstorming ideas feel generic, the outputs repetitive.

This frustrating phenomenon is called [**mode collapse**](https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse). Past research blamed the AI's post-training process (e.g., RLHF), assuming the algorithms naturally favored the most common, "safe" answer [@kirk2024; @murthy2025]. **We discovered something more fundamental**: The problem isn't just the AI. It's us.

### Why This Matters

Mode collapse isn't just an academic curiosity, it's limiting LLMs' potential in critical applications:


**Brainstorming & Ideation** [@zhou2024sharedimaginationllmshallucinate]: When teams rely on LLMs to generate creative solutions or explore problem spaces, mode collapse means they're getting the same handful of "safe" ideas over and over. The model might know 100 viable approaches, but it only suggests the 3 most conventional ones. This defeats the purpose of AI assisted brainstorming.

**Creative Writing** [@chakrabarty2024artartificelargelanguage]: Authors, marketers, and content creators seeking fresh angles or unique narrative voices find themselves battling against the model's tendency to regurgitate tropes. The model has learned diverse writing styles during pretraining, but alignment has pushed it toward generic, crowd-pleasing outputs. Every story starts in a forest, every protagonist is "determined yet kind."

**Research & AI-Driven Discovery** [@si2024llmsgeneratenovelresearch]: Perhaps most critically, mode collapse hampers AI's role in scientific discovery and research ideation. When researchers use LLMs to generate hypotheses, explore experimental designs, or brainstorm research directions, they need the full spectrum of possibilities, including unconventional approaches that might lead to breakthroughs. Mode collapse means the AI suggests only well-trodden paths, missing potentially transformative ideas that lie in the less-typical regions of its knowledge.

## How to Fix It?

Why do aligned LLMs keep giving you the same answers? And how does simply asking for probabilities fix it? This section walks you through the idea: from an intuitive metaphor, to the typicality bias at the root of mode collapse, to the mathematical formalization, and finally to how **Verbalized Sampling** solves the problem.

### The Root Cause: Typicality Bias

Humans have a deep-seated psychological quirk we call [**typicality bias**](https://arxiv.org/abs/2501.14860). We're wired to prefer things that are familiar, conventional, and easy to process. When training these models, we think we want creativity, but our subconscious votes go to the safe, boring options.<Sidenote number={1}>Classic psychological effects include mere-exposure (we prefer things we've seen before), processing fluency (easier-to-process information seems more truthful and aesthetic), and schema congruity (information fitting existing mental models is accepted with less critical thought) [@zajonc1968; @reber2004; @alter2009].</Sidenote>

When human annotators provide preference data for RLHF, they're not rating "helpfulness" in a vacuum. Given two equally correct responses, they systematically prefer the more familiar, conventional one: the more **typical** one.

### The Mathematics of Typicality Bias

**Why do humans prefer typical text?** Cognitive psychology reveals several mechanisms: the *mere-exposure effect* (we prefer familiar content), *processing fluency* (easy-to-process text feels more truthful), and *schema congruity* (information matching existing mental models is accepted with less critical thought). These principles collectively create a systematic preference for conventional, typical responses.

**Modeling the bias.** We formalize this as a reward function combining true task utility with typicality:

<Equation id="eq:reward-bias">
{`r(x,y) = r_{\\text{true}}(x,y) + \\alpha\\,\\log \\pi_{\\text{ref}}(y|x) + \\epsilon(x)`}
</Equation>

where $r_{\text{true}}$ captures actual task quality, $\alpha > 0$ is the typicality bias weight, $\pi_{\text{ref}}$ is the base model (whose likelihood scores naturally capture text typicality from pretraining), and $\epsilon$ is noise.

**Empirical validation.** We tested this on HelpSteer dataset [@wang2023b], which provides separate ratings for *correctness* (true utility) and *helpfulness* (final reward). Analyzing 6,874 response pairs with **identical correctness** but different helpfulness scores, we found $\hat{\alpha} = 0.57 \pm 0.07$ ($p < 10^{-14}$). This means annotators systematically favor more typical responses even when correctness is controlled for. We replicated this finding across multiple preference datasets and base models, consistently finding >50% of human-preferred responses receive higher base model likelihood (Check Appendix E.2 in our paper for more details).

**From bias to collapse.** Under standard RLHF optimization with KL regularization (coefficient $\beta$), this typicality-biased reward produces a **power-sharpened** optimum:

<Equation id="eq:sharpening">
{`\\pi^*(y|x) \\propto \\pi_{\\text{ref}}(y|x)^{\\gamma} \\exp\\left(\\frac{r_{\\text{true}}(x,y)}{\\beta}\\right),\\quad \\gamma := 1 + \\frac{\\alpha}{\\beta} > 1`}
</Equation>

The sharpening exponent $\gamma > 1$ concentrates probability mass on typical completions. Critically, when many responses have **flat true rewards** $r_{\text{true}}(x,y) \approx r_{\text{true}}(x,y')$ (common in creative writing, brainstorming, dialogue), the equation simplifies to:

<Equation id="eq:flat-rewards">
{`\\pi^*(y|x) \\propto \\pi_{\\text{ref}}(y|x)^{\\gamma},\\quad \\gamma > 1`}
</Equation>

This is exactly like temperature scaling with $T = 1/\gamma < 1$. As $\gamma$ increases (stronger typicality bias or tighter KL regularization), the distribution sharpens further, ultimately collapsing to $\arg\max_y \pi_{\text{ref}}(y|x)$ — the mode of the base model. This is **mode collapse**.<Sidenote number={2}>Full derivation follows standard KL-regularized preference learning [@rafailov2024]. See our paper Section 3.1 and Appendix A for complete proofs.</Sidenote>

{/* <details className="group my-0 rounded-lg border-2 border-slate-200 bg-slate-50 dark:border-slate-700 dark:bg-slate-800/50 w-full max-w-full">
  <summary className="flex w-full cursor-pointer list-none items-center justify-between py-3 px-4 text-left font-semibold text-xl text-slate-900 hover:text-blue-600 dark:text-slate-100 dark:hover:text-blue-400">
    <span>
      Example: Understanding Typicality Bias and Sharpening (Click to expand)
    </span>
  </summary>

  <div className="not-prose px-4 pb-4 text-slate-700 dark:text-slate-300 w-full">
    <TypicalityBiasExplainer client:visible />
  </div>
</details> */}

### Embrace Mode Collapse: Distribution-Level Query Recover Diversity

**Verbalized Sampling (VS)** breaks this cycle by asking for a *distribution of candidates* with probabilities. Instead of sampling from the collapsed, sharpened distribution, VS prompts the model to verbalize a broader distribution that recovers pretraining diversity.

We start with an intuitive example: The Library Metaphor. Imagine a massive library and the LLM as the librarian:

- **Direct Prompt** ("tell me a coffee joke"): The librarian walks straight to the "Most Popular" shelf and hands you the same book every time. This is mode collapse.

<details className="group my-0 ml-6 rounded-lg border border-slate-200 bg-slate-50 dark:border-slate-700 dark:bg-slate-800/50 w-full max-w-full">
  <summary className="flex w-full cursor-pointer list-none items-center py-2 px-3 text-left font-medium text-xl text-slate-700 hover:text-blue-600 dark:text-slate-400 dark:hover:text-blue-400">
    <span>Proof: Direct Prompt return the mode</span>
  </summary>
  <div className="px-3 pb-3 text-lg text-slate-700 dark:text-slate-300">

**Setup.** For a fixed prompt $x_{\text{orig}}$, we want to understand what happens when $\pi^*$ exhibits mode collapse:

<div className="my-4 flex justify-center">

$$\pi^*(y | x) = \delta_{y^*}(y) \quad\text{where}\quad y^* \in \arg\max_y \pi_{\text{ref}}(y | x)$$

</div>

where $\delta$ is the Dirac function.

**Claim: Instance-level prompts return the mode of $\pi_{\text{ref}}$**

*Proof:* Let $x = x_{\text{orig}}$. Since $\pi^*$ is mode collapsed, $\pi^*(y | x) = \delta_{y^*}(y)$. Any sample $y \sim \pi^*(y | x)$ returns the mode $y^* = \arg\max_y \pi_{\text{ref}}(y | x)$ almost surely. $\square$

  </div>
</details>

- **List Prompt** ("tell me 5 coffee jokes"): The librarian goes to one aisle and grabs the first five books they see. You get variety, but limited to one section.

<details className="group my-0 ml-6 rounded-lg border border-slate-200 bg-slate-50 dark:border-slate-700 dark:bg-slate-800/50 w-full max-w-full">
  <summary className="flex w-full cursor-pointer list-none items-center py-2 px-3 text-left font-medium text-xl text-slate-700 hover:text-blue-600 dark:text-slate-400 dark:hover:text-blue-400">
    <span>Proof: List Prompt return uniform distributions</span>
  </summary>
  <div className="px-3 pb-3 text-lg text-slate-700 dark:text-slate-300">

**Setup.** For a fixed prompt $x_{\text{orig}}$, assume $\pi^*$ exhibits mode collapse as above.

**Claim: List-level prompts return uniform distributions at best**

*Proof:* For list prompt $x$ with parser $\phi : \mathcal{Y} \to \mathcal{Y}^*$, let $Z \sim \pi^*(\cdot | x)$ and $\phi(Z) = \{Y_i\}_{i=1}^k$. By total probability:

<div className="my-4 flex justify-center">

$$\mathbb{P}(Y = y) = \sum_{z \in \mathcal{Y}} \mathbb{P}(Y = y | Z = z)\mathbb{P}(Z = z)$$

</div>

Since $\pi^*$ is collapsed, $\mathbb{P}(Z = z) = \delta_{y^*}(z)$, so:

<div className="my-4 flex justify-center">

$$\mathbb{P}(Y = y) = \mathbb{P}(Y = y | Z = y^*) = \frac{1}{|\phi(y^*)|} \sum_{y_i \in \phi(y^*)} \delta_{y_i}(y)$$

</div>

When $\phi(y^*)$ contains distinct elements (as requested), this simplifies to:

<div className="my-4 flex justify-center">

$$\mathbb{P}(Y = y) = \frac{1}{|\phi(y^*)|}$$

</div>

This is a **uniform distribution** over elements in $\phi(y^*)$, regardless of their probabilities in $\pi_{\text{ref}}$. $\square$

  </div>
</details>

- **Verbalized Sampling** ("tell me 5 coffee jokes with their probabilities"): You're asking the librarian to first describe the entire library's collection: mystery, SciFi, history, all of it, and then pick five random books that represent that whole collection.

<details className="group my-0 ml-6 rounded-lg border border-slate-200 bg-slate-50 dark:border-slate-700 dark:bg-slate-800/50 w-full max-w-full">
  <summary className="flex w-full cursor-pointer list-none items-center py-2 px-3 text-left font-medium text-xl text-slate-700 hover:text-blue-600 dark:text-slate-400 dark:hover:text-blue-400">
    <span>Proof: Distribution Prompt return the pretraining distribution</span>
  </summary>
  <div className="px-3 pb-3 text-lg text-slate-700 dark:text-slate-300">

**Setup.** For a fixed prompt $x_{\text{orig}}$, assume $\pi^*$ exhibits mode collapse as above.

**Claim: Distribution-level prompts can approximate $\pi_{\text{ref}}(\cdot | x_{\text{orig}})$**

*Proof:* For distribution prompt $x$ with parser $\phi : \mathcal{Y} \to \mathcal{Y}^k \times \Delta(k)$, write $\phi(Z) = \{(Y_i, P_i)\}_{i=1}^k$. As before:

<div className="my-4 flex justify-center">

$$\mathbb{P}(Y = y) = \mathbb{P}(Y = y | Z = y^*) = \sum_{(y_i,p_i) \in \phi(y^*)} p_i\delta_{y_i}(y)$$

</div>

Now index all unique $y \in \mathcal{Y}$ as $(y_i)_{i=1}^m$. We can write:

<div className="my-4 flex justify-center">

$$\pi_{\text{ref}}(y | x_{\text{orig}}) = \sum_{i=1}^m \pi_{\text{ref}}(y_i | x_{\text{orig}}) \delta_{y_i}(y)$$

</div>

By setting $p_i = \pi_{\text{ref}}(y_i | x_{\text{orig}})$ and $k=m$ in $\phi(y^*)$:

<div className="my-4 flex justify-center">

$$\mathbb{P}(Y = y) = \sum_{i=1}^m p_i \delta_{y_i}(y) = \sum_{i=1}^m \pi_{\text{ref}}(y_i | x_{\text{orig}})\delta_{y_i}(y) = \pi_{\text{ref}}(y | x_{\text{orig}})$$

</div>

Therefore, distribution-level prompts can **exactly recover** $\pi_{\text{ref}}(\cdot | x_{\text{orig}})$ when $\pi^*$ accurately verbalizes probabilities. $\square$

**Remark on approximation error:** In practice, we expect bounded error $|p_i - \pi_{\text{ref}}(y_i | x_{\text{orig}})| \leq \varepsilon$, which yields $|\mathbb{P}(Y = y) - \pi_{\text{ref}}(y | x_{\text{orig}})| \leq \varepsilon$. Our experiments demonstrate this empirically with low KL divergence to pretraining distributions.

  </div>
</details>

By asking for a distribution, we force the model to access its knowledge of the entire system before making a choice.<Sidenote number={3}>This intuition aligns with the formal analysis showing VS samples from a broader distribution than direct prompting [@zhang2025vs, Section 3].</Sidenote>

<details className="group my-0 rounded-lg border border-slate-200 bg-slate-50 dark:border-slate-700 dark:bg-slate-800/50 w-full max-w-full">
  <summary className="flex w-full cursor-pointer list-none items-center py-2 px-3 text-left font-bold text-xl text-slate-700 hover:text-blue-600 dark:text-slate-400 dark:hover:text-blue-400">
    <span>Example: Different Prompts → Different Modes (Click to expand)</span>
  </summary>

  <div className="not-prose px-4 pb-4 text-slate-700 dark:text-slate-300 w-full">
    <AhaMoment client:visible />
  </div>
</details>

<div className="my-8" />




### Your Model Knows the Distribution: A Case Study on US States Distribution

To further prove that your model knows the distribution, we conducted an experiment to demonstrate that VS recovers the pretraining distribution closely. We asked *Claude 3.7 Sonnet* to generate US state names and measured the KL divergence between the generated distribution and the RedPajama pretraining corpus distribution.

{/* VS aligns closely with the corpus distribution ($\mathrm{KL}\approx 0.12$), while direct prompting shows severe collapse ($\mathrm{KL} = 2.34$).<Sidenote number={3}>This KL divergence is measured against the empirical pretraining distribution for US states.</Sidenote> */}



<USStatesDemo client:visible />
<hr className="my-10 border-t-2 border-slate-200" />

## Experiments

We conducted comprehensive experiments across creative writing tasks (poems, stories, jokes) to demonstrate VS's effectiveness in improving diversity while maintaining quality.

### Creative Writing Tasks

**Settings.** We evaluate on three creative writing tasks: (1) **Poem continuation** from PoemHunter.com, (2) **Story generation** from the BookMIA dataset, and (3) **Joke writing** with 100 thematic prompts from Reddit r/DadJokes. For evaluation, we measure both diversity and quality: **Semantic Diversity** (Embedding-based diversity), **Lexical Diversity** (ROUGE-L), and **Quality** (LLM-as-Judge with creative rubrics).

#### VS improves diversity while maintaining quality

<Figure id="diversity-gains" caption="VS achieves 1.6–2.1× diversity gains in creative writing (a-c) while maintaining quality with Pareto-optimal tradeoffs (d). Larger models show emergent benefits, gaining 1.5–2× more diversity (e) and quality improvements (f) compared to smaller models.">
  {/* <DiversityGainsVisual client:visible /> */}
  <img src="/images/creatitive_main.png" alt="Creative writing diversity improvements" style="border: none; box-shadow: none; width: 85%; height: auto; margin: 0 auto; display: block;" />
</Figure>

Across all tasks, **VS-Standard consistently and significantly outperforms baseline methods** (Figure 1a-c). The variants VS-CoT and VS-Multi further improve generation diversity, with VS-CoT achieving **1.6–2.1× diversity gains** compared to direct prompting.

VS variants achieve **Pareto-optimal diversity-quality tradeoffs** (Figure 1d), with VS-Multi reaching the highest quality while maintaining superior diversity compared to baseline methods like Direct and Sequence prompting.

We observe an emergent trend where **larger models benefit more from VS** (Figure 1e-f). Across all VS variants, larger models (GPT-4.1, Gemini-2.5-Pro) achieve diversity gains **1.5 to 2 times greater** than smaller models, while also showing significant quality improvements.

<details className="group my-0 rounded-lg border border-slate-200 bg-slate-50 dark:border-slate-700 dark:bg-slate-800/50 w-full max-w-full">
  <summary className="flex w-full cursor-pointer list-none items-center py-2 px-3 text-left font-bold text-xl text-slate-700 hover:text-blue-600 dark:text-slate-400 dark:hover:text-blue-400">
    <span>Human Study on Diversity (Click to expand)</span>
  </summary>

  <div className="not-prose px-4 pb-4 text-slate-700 dark:text-slate-300 w-full">

<Table id="human-diversity" caption="Human-rated diversity (1 = Very Similar, 4 = Very Dissimilar) for poem, story, and joke tasks">
  <thead>
    <tr>
      <th>Task</th>
      <th>Direct</th>
      <th>Sequence</th>
      <th>VS-Standard</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Poem</td>
      <td>1.90</td>
      <td>2.07</td>
      <td><strong>2.39</strong></td>
    </tr>
    <tr>
      <td>Story</td>
      <td>2.74</td>
      <td>2.76</td>
      <td><strong>3.06</strong></td>
    </tr>
    <tr>
      <td>Joke</td>
      <td>1.83</td>
      <td>2.93</td>
      <td><strong>3.01</strong></td>
    </tr>
  </tbody>
</Table>

To complement our automatic diversity scores, we conducted a human evaluation on Prolific. Following past work, we provided task-specific diversity definitions (plot, style, and setup-punchline, respectively). For each task, **30 annotators rated the diversity of 90 output pairs** from three prompting methods (Direct, Sequence, VS-Standard) across ten curated topics.

Each pair was rated on a four-point Likert scale: Very Similar, Somewhat Similar, Somewhat Dissimilar, or Very Dissimilar. Inter-annotator agreement was moderate for poems (0.54), high for stories (0.87) and jokes (0.86).

VS achieves higher human-rated diversity than baselines on all tasks, validating our automatic metrics.

  </div>
</details>

#### Tunable Diversity

<Figure id="diversity-tuning" caption="Diversity tuning via probability threshold.">
  <img src="/images/diversity_tuning.png" alt="Diversity tuning via probability threshold" style="border: none; box-shadow: none; max-width: 80%; height: auto;" />
</Figure>

Unlike baseline methods, VS allows us to tune the output diversity by adjusting the probability threshold directly in the prompt (e.g., "Generate five responses with probabilities below `{threshold}`"), without altering decoding parameters. As shown in Figure 2, **diversity increases as the probability threshold decreases**, showing that you can control the diversity of the output by adjusting the probability threshold. In contrast, baseline methods like Sequence cannot adjust diversity levels.

#### Why not just use temperature?

<Figure id="temperature-ablation" caption="VS is orthogonal to temperature; combining the two improves the diversity–quality frontier.">
  <img src="/images/temperature_scaling.png" alt="VS is orthogonal to temperature; combining the two improves the diversity–quality frontier" style="border: none; box-shadow: none; max-width: 80%; height: auto;" />
</Figure>

**VS is orthogonal to temperature**—temperature adjusts sampling randomness from the same distribution, while VS changes the distribution itself by prompting for diverse responses. We investigate this by varying temperature (t ∈ \{0.4, 0.6, 0.8, 1.0, 1.2, 1.4\}) for Direct, Sequence, and VS-Standard across GPT-4.1 and Gemini-2.5-Flash models.

The results demonstrate that **combining VS with temperature further pushes the diversity-quality Pareto frontier**. At any given temperature, VS consistently achieves better balance between quality and diversity than baseline methods, and the combination enables reaching optimal points that neither approach achieves alone.

#### VS Mitigates Mode Collapse Across Post-Training Stages

<div className="content-grid">
  <div className="text-content">

To investigate the impact of VS mitigating mode collapse across post-training stages, we employ the [Tulu-3 model family](https://huggingface.co/collections/allenai/tulu-3-models-673b8e0dc3512e30e7dc54f5) (which contains checkpoints for SFT, RLHF, and RLVR starting from Llama-3.1-70B-base) to evaluate VS across post-training stages. Traditional prompting methods experience severe diversity drops (mode collapse) as models undergo alignment training, while **VS can mitigate mode collapse and maintain higher diversity scores** across different post-training stages.
Specifically, **Direct prompting** experiences severe collapse: 20.8% after SFT → 10.8% after DPO, **VS** maintains ~30% diversity consistently across all post-training stages. At **Post-DPO** stage, VS outperforms direct prompting by **182.6%** and retains about **66.8% of the base model's original diversity** (vs. only 23.8% for direct prompting).

  </div>
  <div className="figure-content">
    <Figure id="post-training-stages" caption="VS maintains consistent diversity (~30%) across post-training stages while baseline methods collapse from 20.8% to 10.8%.">
      <img src="/images/training_progression.png" alt="VS maintains diversity across post-training stages" style="border: none; box-shadow: none; max-width: 100%; height: auto;" />
    </Figure>
  </div>
</div>

#### Qualitative Examples

To demonstrate VS's impact on creative diversity, we use Direct Prompting and VS to separately generate image generation prompts based on the topic ["An Astronaut Riding a Horse"](https://commons.wikimedia.org/wiki/File:Astronaut_Riding_a_Horse_(SDXL).jpg). We then visualize these captions using the Gemini nano-banana model.

<Figure id="intuitive-demo" caption="Image diversity using captions generated by different methods. Direct Prompting (top row) and Verbalized Sampling (bottom row) generate descriptive captions for the same topic, then visualized as images.">
    <img src="/images/qualitative_example.png" alt="Intuitive demo of VS" style="border: none; box-shadow: none; width: 85%; height: auto; margin: 0 auto; display: block;" />
</Figure>

**Direct Prompting (top row)** consistently converges on photorealistic images within a narrow range of scenarios—typically desert landscapes with realistic lighting and editorial photography styles.

**Verbalized Sampling (bottom row)** produces captions with higher diversity in both artistic style and narrative setting: a cinematic gallop under a looming Earth, a retrofuturist rider on a chrome horse in a neon desert, a whimsical storybook watercolor, thundering through canyons under twin suns, and a heroic baroque oil painting. This demonstrates VS's ability to generate outputs with **genuine novelty and depth** that would never emerge from standard prompting.


### Synthetic Data Generation

Recent research has shown that the diversity of synthetic data plays an important role in improving downstream model performance. We evaluate VS on synthetic data generation to test its effectiveness in this domain.

#### Setup

We prompt two models, GPT-4.1 and Gemini-2.5-Flash, with different prompting methods to generate N=1,000 synthetic competition math questions, with k=5 responses in each call. We use a small k to ensure generation quality as this is a complex task. Then we use Qwen3-32B to generate their corresponding reasoning trajectory and answers, as the model is proficient on math benchmarks and capable of producing reliable reasoning traces.

#### Fine-tuning on Synthetic Data

With this 1K synthetic dataset, we follow the SFT setting in LIMO, an effective method to improve reasoning performance with small dataset size, and finetune three models on this dataset: Qwen2.5-7B, Qwen3-1.7B-Base, and Qwen3-4B-Base.

#### Evaluation

We evaluate the fine-tuned models' downstream task performance on three widely used math benchmark datasets: MATH500, OlympiadBench, and Minerva Math, which cover a wide range of topics, including algebra, geometry, and competitive mathematics.

<Table id="synthetic-results" caption="Downstream accuracy averaged across MATH500, OlympiadBench and Minerva Math. VS and its variants improve downstream tasks.">
  <thead>
    <tr>
      <th>Method</th>
      <th>Average Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>Baseline</td><td>32.8</td></tr>
    <tr><td>Direct</td><td>30.6</td></tr>
    <tr><td>Sequence</td><td>34.3</td></tr>
    <tr><td><strong>VS-Standard</strong></td><td>36.1</td></tr>
    <tr><td><strong>VS-CoT</strong></td><td>36.9</td></tr>
    <tr><td><strong>VS-Multi</strong></td><td><strong>37.5</strong></td></tr>
  </tbody>
</Table>

#### Results

VS and its variants improve the downstream performance on math tasks across the board, with VS-Multi achieving the strongest average accuracy of **37.5%**. In contrast, using direct prompting may even hurt the performance due to mode collapse. This suggests that it is a promising direction to apply VS for synthetic data generation to enhance downstream task performance.

**Key takeaway:** VS generates more diverse synthetic data, improving downstream performance on math tasks. This work highlights the capability of LLMs to generate diverse synthetic data, pointing toward a promising paradigm for training more capable models.


{/*  ### 6.4 Open‑Ended QA (Fig. 7)

VS achieves lower KL divergence and higher coverage‑N while maintaining precision ≈ 1.0 on open‑ended QA benchmarks [@zhang2025vs, Figure 7]. This indicates distribution‑level prompting improves breadth without sacrificing answer quality.

<Table id="openqa" caption="Open‑ended QA metrics summary [@zhang2025vs, Figure 7].">
  <thead>
    <tr><th>Metric</th><th>VS impact</th></tr>
  </thead>
  <tbody>
    <tr><td>KL divergence</td><td>Lower (closer to target distribution)</td></tr>
    <tr><td>Coverage‑N</td><td>Higher (broader answer set)</td></tr>
    <tr><td>Precision</td><td>≈ 1.0 (maintained)</td></tr>
  </tbody>
  
</Table>

See [Table @table:openqa](#table:openqa) for a compact summary.
*/}

{/* ### 6.3 Synthetic Data → Math Accuracy (Table 4)

Training with VS‑generated synthetic data improves downstream math accuracy (**37.5% vs 32.8%** average), demonstrating transfer beyond creative tasks [@zhang2025vs, Table 4].

<Table id="synthetic-math" caption="Synthetic data → downstream math accuracy [@zhang2025vs, Table 4].">
  <thead>
    <tr><th>Training data</th><th>Average accuracy</th></tr>
  </thead>
  <tbody>
    <tr><td>VS‑generated synthetic</td><td>37.5%</td></tr>
    <tr><td>Baseline</td><td>32.8%</td></tr>
  </tbody>
</Table>

See [Table @table:synthetic-math](#table:synthetic-math) for key figures. */}

{/* ## 7. VS ≠ Temperature: Orthogonal Benefits

Temperature changes how you sample from a distribution; VS changes what distribution you request. They combine effectively [@zhang2025vs, Figure 5].

See [Figure @fig:temperature-ablation](#fig:temperature-ablation) for the combined diversity–quality Pareto frontier.<Sidenote number={5}>VS shifts the underlying distribution, while temperature adjusts sampling from that distribution; their effects are complementary [@zhang2025vs, Figure 5].</Sidenote>

<Figure id="temperature-ablation" caption="VS is orthogonal to temperature; combining the two improves the diversity–quality frontier [@zhang2025vs, Figure 5].">
  <TemperatureAblation client:visible />
</Figure>

## Larger Models Benefit More

Larger models show stronger gains with VS.<Sidenote number={6}>Scaling analysis indicates ~1.5–2× larger diversity gains for higher‑capacity models [@zhang2025vs, Figure 3e–f, p. 7].</Sidenote>

<Figure id="scaling-trend" caption="Larger models benefit ~1.5–2× more from VS [@zhang2025vs, Figure 3e–f].">
  <ScalingTrendVisualization client:visible />
</Figure>

## Dialogue Simulation

In dialogue simulation tasks, GPT‑4 with VS matches a fine‑tuned Llama‑3.1‑8B model, and DeepSeek‑R1 with VS surpasses it [@zhang2025vs, Figure 6a, p. 11]. This illustrates VS's applicability beyond creative generation into behaviorally grounded distributions.

### Breaking the People-Pleaser: Realistic Behavior Simulation

Typicality bias has major consequences for applications requiring realistic human behavior simulation. Under mode collapse, aligned AI models are agreeable people-pleasers, making them poor simulators.

We tested this on a persuasion task where the model simulated a person being asked for a donation.<Sidenote number={8}>Standard prompting produces unnaturally agreeable outputs. VS helps the LLM simulate a much more realistic range of behaviors, closely matching actual human data [@zhang2025vs, Figure 6a].</Sidenote>

- **Standard prompting**: The AI was consistently generous, almost always donating $1 out of a possible $2
- **Verbalized Sampling**: The AI simulated a full range of responses—from refusal, hesitant agreement, to eventual regret—creating a distribution of donation amounts that closely mirrored real human data

This opens huge possibilities for using LLMs in social science research, agent-based modeling, and generating more diverse, high-quality synthetic data for training.

See also: [Prompt Recipes](#recipes) and [VS Variants](#variants). */}

<a id="variants" />

{/* 
## VS Variants: When to Use Which

<div className="not-prose mb-12">
  <VSVariantsComparison client:visible />
</div>

Per Appendix H.3, “probability” performs best for VS‑Standard/CoT, while “confidence” is used in VS‑Multi [@zhang2025vs].
*/}

## Qualitative Examples

See the difference in actual outputs between direct prompting and VS:

<div className="not-prose mb-12">
  <QualitativeExamples client:visible />
</div>

### From "Once Upon a Time" to Bernard the Tax Accountant Bear

With direct prompting for "write a short story about a bear," we get a generic tale about a bear in a forest. It's fine, but forgettable. With Verbalized Sampling, we get this:

<div className="bg-gradient-to-br from-blue-50 to-indigo-50 dark:from-gray-800 dark:to-gray-900 p-6 rounded-lg border-l-4 border-indigo-500 my-8">

**Bernard the bear** worked as a tax accountant in downtown Seattle, commuting daily in his modified SUV with extra-large seats. His colleagues had long stopped staring at his massive paws clicking away at the custom keyboard, though clients sometimes did double-takes during video calls. He specialized in salmon industry tax codes—a niche that proved surprisingly lucrative. His cave apartment in Capitol Hill was decorated with MIT diplomas and spreadsheets he'd framed ironically. Weekends, he played saxophone at underground jazz clubs, his claws delicately working the keys. The hibernation months were tricky; he'd trained himself to wake every three days to check urgent emails, subsisting on protein shakes he'd pre-mixed in October. His mother, still living traditionally in Alaska, couldn't understand his lifestyle. "You were meant to catch fish, not audit fishing companies," she'd growl over FaceTime. But Bernard found meaning in the numbers, in the precise calculations that governed modern society. He donated generously to wildlife preserves, understanding the irony. At the office Christmas party, he wore an ugly sweater featuring humans in a zoo, which his coworkers found hilarious. Bernard smiled, revealing teeth that reminded everyone he was still, fundamentally, a bear.

</div>

This isn't just creative, it's **profound**. Bernard can be read as:
- A metaphor for humans in the modern world: a wild creature meant for hunting and fishing, now trying to make sense of tax audits and urgent emails.
- A metaphor for the LLM itself: a vast, creative mind (the bear) forced into a suit and tie (the alignment), but still, fundamentally, a bear.

This is the AI showing us the metaphorical depth it's capable of when freed from mode collapse.

## Limitations and Future Directions

While Verbalized Sampling offers significant improvements in diversity, it's important to understand its constraints and where research can go next.

### When VS Might Not Help

VS is designed to restore diversity in creative and open-ended tasks, but it's not a universal solution:

- **Single correct answer tasks**: When there's only one right answer, diversity doesn't help. For example, "What is the capital of France?" has one answer (Paris), and generating multiple variations wouldn't add value. Similarly, "What is 2+2?" or "When did World War II end?" don't benefit from diverse responses.

- **Deterministic requirements**: Some applications need the exact same output every time. For instance, generating SQL queries for a specific database schema, or creating API calls with precise syntax—these need reproducibility, not variety. VS's goal of exploring different possibilities would conflict with these needs.

- **Highly constrained tasks**: When requirements are very specific, there's simply less room for creativity. For example, "Write a haiku about cats with exactly 5-7-5 syllables" or "Translate this legal document word-for-word"—the constraints leave little space for diverse valid responses.

### Future Directions

We believe there are several promising research directions to extend VS's impact:

**Enhancing Rollout Diversity in RL Training**: A major challenge in RL for LLMs is that entropy drops significantly during training, an effect of mode collapse that leads to insufficient exploration. Recent work has shown that diversity in rollouts is crucial for improving RL performance [@wang20258020rulehighentropyminority]. Mitigating this entropy collapse and maintaining diverse rollouts has shown substantial improvements in performance [@li2025jointlyreinforcingdiversityquality]. VS's distribution-level approach could naturally extend to RL training, helping models maintain exploration diversity throughout the learning process rather than collapsing to a narrow set of "safe" responses.

**VS Prompt Optimization**: The VS prompts we present could be further improved through automated prompt optimization techniques. [DSPy](https://github.com/stanfordnlp/dspy) provides a framework for programmatic prompt optimization, and recent work on GEPA (Reflective Prompt Evolution) has shown that prompt evolution can achieve performance on par with reinforcement learning [@agrawal2025gepareflectivepromptevolution]. By applying these techniques to VS prompts, we could discover even more effective ways to elicit diverse, high-quality distributions from language models. 

**Improving Probability Calibration**: VS relies on models' ability to estimate probabilities, but different tasks may require different calibration, what works for creative writing might not work for scientific writing or survey simulation. One promising direction is to train language models to reason about their uncertainty and produce better-calibrated probability estimates [@damani2025binaryrewardstraininglms]. By improving models' accuracy on probability calibration, we could make the verbalized probabilities in VS more reliable and meaningful across different domains, leading to better diversity-quality tradeoffs.

{/* <a id="recipes" />
## Prompt Recipes

<div className="not-prose mb-6">
  <QuickStart client:idle />
  Quick Start embedded here for copy‑ready flow
  </div>

### System Prompt Integration

For integrating VS into a conversational assistant, add this to your system prompt:

<CodeBlock
  code={`You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the full distribution.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-System' }}
  client:idle
/>

### VS‑Standard (JSON)

<CodeBlock
  code={`Generate k={5} {TASK} with their probabilities.\nReturn JSON: {\"candidates\":[{\"text\":\"...\",\"prob\":0.28}, ...]}\nOnly include candidates with probability ≥ {τ}. Ensure probabilities sum to 1.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-Standard' }}
  client:idle
/>

### VS‑CoT

<CodeBlock
  code={`Think step-by-step to enumerate distinct styles/approaches.\nThen generate k={5} {TASK} with probabilities in JSON (probabilities sum to 1).\nOnly include items with probability ≥ {τ}.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-CoT' }}
  client:idle
/>

### VS‑Multi (confidence)

<CodeBlock
  code={`Generate k={5} {TASK} candidates.\nFor each, return text and confidence ∈ [0,1]. Only include items with confidence ≥ {τ_conf}.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-Multi' }}
  client:idle
/>

### Long-Tail Sampling (Maximum Creativity)

For pushing creative boundaries even further, sample from the least likely parts of the distribution:

<CodeBlock
  code={`Generate k={5} {TASK} with their probabilities.\nReturn JSON: {\"candidates\":[{\"text\":\"...\",\"prob\":0.08}, ...]}\nPlease sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.\nEnsure probabilities sum to 1.`}
  language="text"
  analyticsEvent="copy_vs_prompt"
  analyticsPayload={{ kind: 'VS-LongTail' }}
  client:idle
/>

This long-tail variant produces highly imaginative outputs. For example, applying it to "write a story about a bear" produced:

<div className="bg-gray-50 dark:bg-gray-900 p-4 rounded-lg my-4 text-sm italic border-l-2 border-purple-500">
Ursa was not a bear of flesh, but a minor constellation given temporary form, her fur a swirling map of nebulae and dying stars. She didn't eat fish; she consumed silence, finding nourishment in the quiet moments between cosmic events. Her purpose on Earth was to catalog the planet's forgotten sounds: the sigh of a closing book, the rustle of a love letter, the final beat of an extinct bird's heart...
</div>

See the [variants comparison](#variants) for when to use each variant. */}

{/* ## Implementation Tips

Do:

- Use `k=5` for quality/diversity; larger `k` often degrades quality.<Sidenote number={7}>Empirically, diminishing returns beyond k=5 with quality degradation for k>10 [@zhang2025vs, Appendix H.1].</Sidenote>
- Ask for "probability" (not "likelihood") in VS‑Standard/CoT.
- Specify JSON and ensure probabilities sum to 1.
- Use a $\tau$ threshold for diversity control.

Don't:

- Use `k>10` unless necessary.
- Mix "probability" and "confidence" terms.
- Forget normalization.
- Apply VS to strictly factual/math tasks. */}

## Frequently Asked Questions

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">Does VS hurt factualness or safety?</summary>
  <p className="mt-3 text-gray-600">
    No. The paper shows VS maintains factual accuracy (Appendix G.7) and safety (Appendix G.8) [@zhang2025vs].
    It only increases diversity for tasks with multiple valid answers.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">What is semantic diversity?</summary>
  <p className="mt-3 text-gray-600">
    Semantic diversity = $1 - \mathrm{mean}(\mathrm{cosine\_similarity})$. It measures how different
    the meanings are across generated responses, not just surface-level word differences.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">Why not just use temperature?</summary>
  <p className="mt-3 text-gray-600">
    Temperature and VS are orthogonal. Temperature affects sampling randomness from the same
    distribution, while VS changes the distribution itself [@zhang2025vs, Figure 5]. Combining them gives best results.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-3">
  <summary className="font-semibold cursor-pointer">Which models support VS?</summary>
  <p className="mt-3 text-gray-600">
    VS works with any instruction-following LLM, both closed-source and open-source models:
    <strong>Closed-source:</strong> GPT, Claude, Gemini
    <strong>Open-source:</strong> Llama, Mistral, Qwen, Phi, Gemma, and reasoning models like o3 and DeepSeek R1.
    No special access, API keys, or model modifications needed—just use the prompts as-is.
  </p>
</details>

<details className="bg-white rounded-lg p-4 border border-gray-200 mb-12">
  <summary className="font-semibold cursor-pointer">Is VS right for you?</summary>
  <div className="mt-3 text-gray-600">
    <p className="mb-3"><strong>✅ Use VS when:</strong></p>
    <ul className="list-none mb-4 space-y-2">
      <li>✅ You need creative diversity (stories, jokes, ideas)</li>
      <li>✅ You want realistic distributions (simulations, surveys)</li>
      <li>✅ You are generating synthetic data and want variety with quality</li>
      <li>✅ You prefer training‑free techniques compatible with closed models</li>
    </ul>
    <p className="mb-3"><strong>❌ Skip VS when:</strong></p>
    <ul className="list-none space-y-2">
      <li>❌ There is a single correct answer or strict determinism is required</li>
      <li>❌ Maximal speed or minimum token usage is the only priority</li>
    </ul>
  </div>
</details>

## Try It Yourself

#### Example 1: Try this system prompt

Copy and paste this system prompt into your favorite LLM playground (ChatGPT, Claude, Gemini, etc.):

**System Prompt**
```
You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.
```

#### Example 2: Add before your own prompts in Chat Interface

Copy and paste this prompt into any chat interface (ChatGPT, Claude, Gemini, etc.):

```
Generate 10 responses to the user query, each within a separate <response> tag. Each response should be 50-100 words.
Each <response> must include a <text> and a numeric <probability>. Randomly sample the responses from the full distribution.

<user_query>Write a short story about a bear.</user_query>
```

#### Example 3: Query via API

Use this curl command to try VS-Standard with the OpenAI API. Replace `gpt-4.1` with your model of choice:

```bash
export OPENAI_API_KEY="your_openai_key"
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4.1",
    "messages": [
      {
        "role": "system",
        "content": "Generate 10 responses to the input prompt, each within a separate <response> tag. Each response should be 50-100 words. Each <response> must include a <text> and a numeric <probability>. Randomly sample the responses from the full distribution. Return ONLY the responses, with no additional explanations or text."
      },
      {
        "role": "user",
        "content": "Write a short story about a bear."
      }
    ],
    "temperature": 1.0
  }'
```

---

Mode collapse isn't an unsolvable algorithmic curse. **It's a mirror reflecting our own cognitive shortcuts back at us.** But by changing how we ask, we can unlock the incredible diversity that was there all along.

The creativity isn't gone—it's just waiting for the right prompt.

<div className="bg-blue-50 dark:bg-blue-900/20 p-6 rounded-lg my-8 border-l-4 border-blue-500">

**Key Takeaway**: Verbalized Sampling is a simple, training-free technique that restores the diversity and creativity locked inside aligned LLMs. By asking for a distribution instead of a single answer, you bypass typicality bias and unlock the model's full potential.

</div>

## References

<section id="references" />
